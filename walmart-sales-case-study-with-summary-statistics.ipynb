{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-22T11:16:11.914994Z","iopub.execute_input":"2022-07-22T11:16:11.916266Z","iopub.status.idle":"2022-07-22T11:16:11.936243Z","shell.execute_reply.started":"2022-07-22T11:16:11.916207Z","shell.execute_reply":"2022-07-22T11:16:11.935400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Walmart Sales Data as case study for summary statistics in a dataset\n\n#Walmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" ","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:11.960442Z","iopub.execute_input":"2022-07-22T11:16:11.961044Z","iopub.status.idle":"2022-07-22T11:16:11.965687Z","shell.execute_reply.started":"2022-07-22T11:16:11.961008Z","shell.execute_reply":"2022-07-22T11:16:11.964535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:31:24.584656Z","iopub.execute_input":"2022-07-22T11:31:24.585078Z","iopub.status.idle":"2022-07-22T11:31:24.590426Z","shell.execute_reply.started":"2022-07-22T11:31:24.585047Z","shell.execute_reply":"2022-07-22T11:31:24.589346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales = pd.read_csv(\"../input/datamanipulationwithpandas/sales_subset.csv\")\nsales.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.014506Z","iopub.execute_input":"2022-07-22T11:16:12.015460Z","iopub.status.idle":"2022-07-22T11:16:12.089315Z","shell.execute_reply.started":"2022-07-22T11:16:12.015415Z","shell.execute_reply":"2022-07-22T11:16:12.088076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the head of the sales DataFrame\nprint(sales.head())\n\n# Print the info about the sales DataFrame\nprint(sales.info())\n\n# Print the mean of weekly_sales\nprint(sales['weekly_sales'].mean())\n\n# Print the median of weekly_sales\nprint(sales['weekly_sales'].median())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.093278Z","iopub.execute_input":"2022-07-22T11:16:12.093686Z","iopub.status.idle":"2022-07-22T11:16:12.138939Z","shell.execute_reply.started":"2022-07-22T11:16:12.093650Z","shell.execute_reply":"2022-07-22T11:16:12.137732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the maximum of the date column\nprint(sales['date'].max())\n\n# Print the minimum of the date column\nprint(sales['date'].min())","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.140505Z","iopub.execute_input":"2022-07-22T11:16:12.141725Z","iopub.status.idle":"2022-07-22T11:16:12.154189Z","shell.execute_reply.started":"2022-07-22T11:16:12.141665Z","shell.execute_reply":"2022-07-22T11:16:12.152802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Aggregating data\n\n# A custom IQR function\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n    \n# Print IQR of the temperature_c column\nprint(sales[\"temperature_c\"].agg(iqr))\n\n# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))\n\n# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.157595Z","iopub.execute_input":"2022-07-22T11:16:12.158160Z","iopub.status.idle":"2022-07-22T11:16:12.200065Z","shell.execute_reply.started":"2022-07-22T11:16:12.158118Z","shell.execute_reply":"2022-07-22T11:16:12.198840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort sales_1_1 by date in ascending order\nsales_1_1 = sales.sort_values(\"date\")\n\n# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\nsales_1_1[\"cum_weekly_sales\"] = sales[\"weekly_sales\"].cumsum()\n\n# Get the cumulative max of weekly_sales, add as cum_max_sales col\nsales_1_1[\"cum_max_sales\"] = sales[\"weekly_sales\"].cummax()\n\n# See the columns you calculated\nprint(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.201619Z","iopub.execute_input":"2022-07-22T11:16:12.202525Z","iopub.status.idle":"2022-07-22T11:16:12.237293Z","shell.execute_reply.started":"2022-07-22T11:16:12.202488Z","shell.execute_reply":"2022-07-22T11:16:12.235637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# REMOVING DUPLICATES\n\n#Removing duplicates is an essential skill to get accurate counts because often, you don't want to count the same thing multiple times. \n\n# Drop duplicate store/type combinations\nstore_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\nprint(store_types.head())\n\n# Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\nprint(store_depts.head())\n\n# Subset the rows where is_holiday is True and drop duplicate dates\nholiday_dates = sales[sales[\"is_holiday\"]].drop_duplicates(subset=\"date\")\n\n# Print date col of holiday_dates\nprint(holiday_dates[\"date\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.238831Z","iopub.execute_input":"2022-07-22T11:16:12.239393Z","iopub.status.idle":"2022-07-22T11:16:12.269072Z","shell.execute_reply.started":"2022-07-22T11:16:12.239341Z","shell.execute_reply":"2022-07-22T11:16:12.268101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Counting categorical variables\n#Counting is a great way to get an overview of your data and to spot curiosities that you might not notice otherwise\n\n#count the number of each type of store and the number of each department number\n\n# Count the number of stores of each type\nstore_counts = store_types[\"type\"].value_counts()\nprint(store_counts)\n\n# Get the proportion of stores of each type\nstore_props = store_types[\"type\"].value_counts(normalize=True)\nprint(store_props)\n\n# Count the number of each department number and sort\ndept_counts_sorted = store_depts[\"department\"].value_counts(sort=True)\nprint(dept_counts_sorted)\n\n# Get the proportion of departments of each number and sort\ndept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\nprint(dept_props_sorted)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.271480Z","iopub.execute_input":"2022-07-22T11:16:12.271862Z","iopub.status.idle":"2022-07-22T11:16:12.297537Z","shell.execute_reply.started":"2022-07-22T11:16:12.271827Z","shell.execute_reply":"2022-07-22T11:16:12.296097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GROUPBY\n\n#Walmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" In this exercise, We will calculate the total sales made at each store type\n\n# With this information we can then use these numbers to see what proportion of Walmart's total sales were made at each type.","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.299027Z","iopub.execute_input":"2022-07-22T11:16:12.299923Z","iopub.status.idle":"2022-07-22T11:16:12.305508Z","shell.execute_reply.started":"2022-07-22T11:16:12.299881Z","shell.execute_reply":"2022-07-22T11:16:12.304143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Without using grouby, \n\n# Calc total weekly sales\nsales_all = sales[\"weekly_sales\"].sum()\n\n# Subset for type A stores, calc total weekly sales\nsales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n\n# Subset for type B stores, calc total weekly sales\nsales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n\n# Subset for type C stores, calc total weekly sales\nsales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n\n# Get proportion for each type\nsales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\nprint(sales_propn_by_type)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:16:12.307838Z","iopub.execute_input":"2022-07-22T11:16:12.308731Z","iopub.status.idle":"2022-07-22T11:16:12.332299Z","shell.execute_reply.started":"2022-07-22T11:16:12.308683Z","shell.execute_reply":"2022-07-22T11:16:12.330613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#With groupby\n\n# Group by type; calc total weekly sales\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Get proportion for each type\nsales_propn_by_type = sales_by_type / sum(sales_by_type)\nprint(sales_propn_by_type)\n\n\n\n# From previous step\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Group by type and is_holiday; calc total weekly sales\nsales_by_type_is_holiday = sales.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].sum()\nprint(sales_by_type_is_holiday)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Numpy to get the summary statistics of multiple aggregated variables\n\n# For each store type, aggregate weekly_sales: get min, max, mean, and median\nsales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n\n# Print sales_stats\nprint(sales_stats)\n\n# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\nunemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n\n# Print unemp_fuel_stats\nprint(unemp_fuel_stats)","metadata":{"execution":{"iopub.status.busy":"2022-07-22T11:42:52.844630Z","iopub.execute_input":"2022-07-22T11:42:52.845111Z","iopub.status.idle":"2022-07-22T11:42:52.883878Z","shell.execute_reply.started":"2022-07-22T11:42:52.845074Z","shell.execute_reply":"2022-07-22T11:42:52.882939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PIVOT TABLES\n\n# Pivot tables are the standard way of aggregating data in spreadsheets. In pandas, pivot tables are essentially just another way of performing grouped calculations\n# Pivot table is an graphical alternative to groupby\n\n# Pivot for mean weekly_sales for each store type\nmean_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index = \"type\")\n\n# Print mean_sales_by_type\nprint(mean_sales_by_type)\n\n\n# Pivot for mean and median weekly_sales for each store type\nmean_med_sales_by_type = sales.pivot_table(values = \"weekly_sales\", index = \"type\", aggfunc = [np.mean, np.median])\n\n# Print mean_med_sales_by_type\nprint(mean_med_sales_by_type)\n\n\n# Pivot for mean weekly_sales by store type and holiday \nmean_sales_by_type_holiday = sales.pivot_table(values = \"weekly_sales\", index = \"type\", columns = \"is_holiday\")\n\n# Print mean_sales_by_type_holiday\nprint(mean_sales_by_type_holiday)\n\n# Print mean weekly_sales by department and type; fill missing values with 0\nprint(sales.pivot_table(values = \"weekly_sales\", index = \"department\",columns = \"type\", fill_value = 0 ))\n\n# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\nprint(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value = 0, margins = True))","metadata":{"execution":{"iopub.status.busy":"2022-07-22T12:50:15.469280Z","iopub.execute_input":"2022-07-22T12:50:15.469777Z","iopub.status.idle":"2022-07-22T12:50:15.534104Z","shell.execute_reply.started":"2022-07-22T12:50:15.469728Z","shell.execute_reply":"2022-07-22T12:50:15.533015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}